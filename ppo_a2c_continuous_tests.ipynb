{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SXDmd7m5anf"
   },
   "source": [
    "# PPO and A2C\n",
    "\n",
    "**Note** : this script is inspired from the 1st assignment (without correction) from the RL course of the MVA master by A. Lazaric and M. Pirotta, on finite MDP and function approximation, which required to complete a partial implementation of A2C for discrete action space. It has been extended to include a different critic and actor architecture, continuous action space, and the clipped and adaptative KL losses required for PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "ddg4xczTsPbO",
    "outputId": "bf3678bb-1492-496b-c401-ba54a0c3411a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "try :\n",
    "    import Box2D\n",
    "except :\n",
    "    !pip install Box2D\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "id": "7OwXjxUnsUuK",
    "outputId": "c8eb7564-dbf8-4803-f772-a7909bc78d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script running locally\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd /content/drive/My\\ Drive/RL-PPO\n",
    "except :\n",
    "    print(\"Script running locally\")\n",
    "from config import reset_config, get_arguments\n",
    "from utils import plot_sumup\n",
    "# from ppo import PPOAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "7BUBCp2NsPbc",
    "outputId": "895d6205-f69e-4ed0-a790-7fc2f51f5726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config : \n",
      "\n",
      "{'batch_size': 128,\n",
      " 'beta_KL': 3,\n",
      " 'c1': 1,\n",
      " 'c2': 0.001,\n",
      " 'color': {'A2C_loss': (0.4, 0.7607843137254902, 0.6470588235294118),\n",
      "           'adaptative_KL_loss': (0.9882352941176471,\n",
      "                                  0.5529411764705883,\n",
      "                                  0.3843137254901961),\n",
      "           'clipped_loss': (0.5529411764705883,\n",
      "                            0.6274509803921569,\n",
      "                            0.796078431372549)},\n",
      " 'd_targ': 0.01,\n",
      " 'env': 'BipedalWalker-v3',\n",
      " 'epochs': 1,\n",
      " 'eps_clipping': 0.2,\n",
      " 'gamma': 0.99,\n",
      " 'lambda': 1,\n",
      " 'loss_name': 'clipped_loss',\n",
      " 'lr': 0.0003,\n",
      " 'max_episodes': 1000,\n",
      " 'max_steps': 300,\n",
      " 'optimize_every': 128,\n",
      " 'randomize_batch': False,\n",
      " 'reset_val': None,\n",
      " 'reward_norm': False,\n",
      " 'seed': 42,\n",
      " 'solved_reward': {'CartPole-v1': 300,\n",
      "                   'LunarLander-v2': 230,\n",
      "                   'MountainCar-v0': 300,\n",
      "                   'MountainCarContinuous-v0': 300},\n",
      " 'std': 0.5}\n"
     ]
    }
   ],
   "source": [
    "def reset_config(print_=False):\n",
    "    config = {}\n",
    "    config['env'] = \"BipedalWalker-v3\" ## to choose\n",
    "#     config['env'] = 'MountainCarContinuous-v0'\n",
    "    config['std'] = 0.5 # use constant standard deviation for continuous action space (for now)\n",
    "    config['gamma'] = 0.99 #Discount rate\n",
    "    config['lambda'] = 1 # parameter of the generalized advantage estimation\n",
    "    config['lr'] = 0.0003\n",
    "    config['eps_clipping'] = 0.2 #range : 0.1-0.3\n",
    "    config['d_targ'] = 0.01\n",
    "    config['beta_KL'] = 3\n",
    "    config['c1'] = 1 #paramter of the value function loss\n",
    "    config['c2'] = 1e-3 #entropy parameter --> 1e-4 to 1e-2\n",
    "    config[\"reward_norm\"]=False \n",
    "    config['epochs'] = 1\n",
    "    config['max_episodes'] = 1000\n",
    "    config['max_steps'] = 300\n",
    "    config['optimize_every'] = 128\n",
    "    config['batch_size'] = 128\n",
    "    config[\"randomize_batch\"]=False\n",
    "    # config['buffer_size'] = 2048 #2048 - 409600 /!\\ multiple of the batch size\n",
    "    config['loss_name'] = [\"A2C_loss\",\"adaptative_KL_loss\",\"clipped_loss\"][2]\n",
    "    config['color'] = {\"A2C_loss\":sns.color_palette(\"Set2\")[0],\"adaptative_KL_loss\":sns.color_palette(\"Set2\")[1],\"clipped_loss\":sns.color_palette(\"Set2\")[2]}\n",
    "\n",
    "    config['seed'] = 42\n",
    "    config[\"reset_val\"] = None # use to reset the environment with a custom value\n",
    "    config[\"solved_reward\"] = {'LunarLander-v2':230,\n",
    "                              'MountainCarContinuous-v0':300,\n",
    "                              'CartPole-v1':300,\n",
    "                              'MountainCar-v0':300}\n",
    "    \n",
    "    if print_== True :\n",
    "        print(\"Training config : \\n\")\n",
    "        pprint(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "config = reset_config(print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gym\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from memory import Memory\n",
    "# from networks import CustomValueNetwork, CustomDiscreteActorNetwork, ContinuousActorNetwork\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\n",
    "\t\tself.config = config\n",
    "\t\tself.memory = Memory()\n",
    "\t\tself.device = 'cpu'\n",
    "\t\tself.env = gym.make(config['env'])\n",
    "\n",
    "\t\t# boolean for discrete action space:\n",
    "\t\tself.discrete_action_bool = isinstance(self.env.action_space, Discrete)\n",
    "\t\tself.gamma = config['gamma']\n",
    "\t\tself.lambd = config['lambda']\n",
    "\t\tself.c1 = config['c1']\n",
    "\t\tself.c2 = config['c2']\n",
    "\t\tself.norm_reward = config[\"reward_norm\"]\n",
    "\t\tself.loss_name = config['loss_name']\n",
    "\t\tself.beta_kl = config['beta_KL']\n",
    "\n",
    "\t\tself.batch_size = config[\"batch_size\"]\n",
    "\t\tif not(self.discrete_action_bool):\n",
    "\t\t\tprint(\"Low : \", self.env.action_space.low)\n",
    "\t\t\tprint(\"High : \", self.env.action_space.high)\n",
    "\n",
    "\t\t# set random seeds\n",
    "\t\tnp.random.seed(config['seed'])\n",
    "\t\ttorch.manual_seed(config['seed'])\n",
    "\t\tself.env.seed(config['seed'])\n",
    "\n",
    "\t\t# Critic\n",
    "\t\tself.value_network = CustomValueNetwork(self.env.observation_space.shape[0], 64, 1).to(self.device)\n",
    "\t\tself.value_network_optimizer: optim.Optimizer = optim.Adam(\n",
    "\t\t\tself.value_network.parameters(), lr=config['lr'])\n",
    "\n",
    "\t\t# Actor\n",
    "\t\tif self.discrete_action_bool:\n",
    "\t\t\tself.actor_network = CustomDiscreteActorNetwork(self.env.observation_space.shape[0], 64, self.env.action_space.n).to(self.device)\n",
    "\t\telse:\n",
    "\t\t\tself.actor_network = ContinuousActorNetwork(self.env.observation_space.shape[0], 64, self.env.action_space.shape[0], self.config[\"std\"], self.env).to(self.device)\n",
    "\n",
    "\t\tself.actor_network_optimizer: optim.Optimizer = optim.Adam(\n",
    "\t\t\tself.actor_network.parameters(), lr=config['lr'])\n",
    "\n",
    "\t\t# save in memory policy estimates\n",
    "\t\tself.probs_list = []    # probability of actions taken\n",
    "\t\tself.mean_list = []     # mean estimate (for continuous action)\n",
    "\n",
    "\tdef _returns_advantages(self, values, next_value):\n",
    "\t\t\"\"\"Returns the cumulative discounted rewards with GAE\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\trewards : array\n",
    "\t\t\tAn array of shape (batch_size,) containing the rewards given by the env\n",
    "\t\tdones : array\n",
    "\t\t\tAn array of shape (batch_size,) containing the done bool indicator given by the env\n",
    "\t\tvalues : array\n",
    "\t\t\tAn array of shape (batch_size,) containing the values given by the value network\n",
    "\t\tnext_value : float\n",
    "\t\t\tThe value of the next state given by the value network\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\treturns : array\n",
    "\t\t\tThe cumulative discounted rewards\n",
    "\t\tadvantages : array\n",
    "\t\t\tThe advantages\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\trewards = np.array(self.memory.rewards)\n",
    "\t\tif self.norm_reward:\n",
    "\t\t\trewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "\t\treturns, advantages = [], []\n",
    "\t\tlast = next_value\n",
    "\t\tgae = 0\n",
    "\n",
    "\t\tfor i in reversed(range(len(self.memory))):\n",
    "\t\t\t# build the returns\n",
    "\t\t\treturns.insert(0, rewards[i] + self.gamma*last*(1-self.memory.dones[i]))\n",
    "\n",
    "\t\t\t# build the advantages\n",
    "\t\t\tdelta = rewards[i] + self.gamma*next_value*(1-self.memory.dones[i]) - values[i]\n",
    "\t\t\tgae = delta + self.gamma*self.lambd*(1-self.memory.dones[i])*gae\n",
    "\t\t\tadvantages.insert(0, gae)\n",
    "\t\t\tnext_value = values[i]\n",
    "\n",
    "\t\treturns = torch.FloatTensor(returns).to(self.device)\n",
    "\t\tadvantages = torch.FloatTensor(advantages).to(self.device)\n",
    "\n",
    "\t\treturn returns, advantages\n",
    "\n",
    "\tdef training(self, epochs, optimize_every, max_episodes, max_steps):\n",
    "\t\tt1 = datetime.datetime.now()\n",
    "\t\t\"\"\"Perform a training by batch\n",
    "\t\t\tParameters\n",
    "\t\t\t----------\n",
    "\t\t\tepochs : int\n",
    "\t\t\t\tNumber of epochs\n",
    "\t\t\tbatch_size : int\n",
    "\t\t\t\tThe size of a batch\"\"\"\n",
    "\n",
    "\t\tepisode_count = 0\n",
    "\t\ttimestep_count = 0\n",
    "\t\trewards_test = []\n",
    "\t\tsolved = False\n",
    "\n",
    "\t\tloss_evol = {'loss': [], 'dry_loss': [], 'entropy': []}\n",
    "\t\tif self.loss_name not in [\"A2C_loss\", \"adaptative_KL_loss\", \"clipped_loss\"]:\n",
    "\t\t\tprint('Unknown loss function, using clipped loss as default loss')\n",
    "\t\telse:\n",
    "\t\t\tprint('Loss : ', self.loss_name)\n",
    "\n",
    "\t\tfor ep in range(max_episodes):\n",
    "\t\t\tprint(\"episode number \"+str(ep)+\"/\"+str(max_episodes))\n",
    "\t\t\tif not solved:\n",
    "\t\t\t\tepisode_count += 1\n",
    "\t\t\t\tobs = self.env.reset()\n",
    "\n",
    "\t\t\t\tfor i in range(max_steps):\n",
    "\t\t\t\t\ttimestep_count += 1\n",
    "\t\t\t\t\t# just observed s_t\n",
    "\t\t\t\t\tself.memory.observations.append(obs)\n",
    "\t\t\t\t\t#print(self.memory.observations[0].shape)\n",
    "\t\t\t\t\t# tensor\n",
    "\t\t\t\t\tobs_t = torch.from_numpy(obs).float().to(self.device)\n",
    "\t\t\t\t\t# act on just observed, action a_t\n",
    "\t\t\t\t\taction = self.actor_network.select_action(obs_t.view(1,-1))\n",
    "# \t\t\t\t\tprint(obs_t.view(1,-1).size())\n",
    "# \t\t\t\t\tprint(action)\n",
    "\n",
    "\t\t\t\t\tif self.discrete_action_bool:\n",
    "\t\t\t\t\t\taction = int(action)\n",
    "\t\t\t\t\tself.memory.actions.append(action)\n",
    "\n",
    "\t\t\t\t\t# Run a step : get new state s_{t+1} and rewards r_t\n",
    "\t\t\t\t\t#print(action)\n",
    "\t\t\t\t\tobs, reward, done, _ = self.env.step(action.view(-1))\n",
    "\n",
    "\t\t\t\t\t# Store termination status reward\n",
    "\t\t\t\t\tself.memory.dones.append(done)\n",
    "\t\t\t\t\tself.memory.rewards.append(reward)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\t\tif (timestep_count % optimize_every) == 0:\n",
    "\t\t\t\t\t\t# print(\"starting optim\")\n",
    "\n",
    "\t\t\t\t\t\tfor epoch in range(epochs):\n",
    "\t\t\t\t\t\t\t# print(\"epoch\",epoch)\n",
    "\t\t\t\t\t\t\tloss_val, dry_loss_val, entrop_val = self.optimize_model(obs)\n",
    "\t\t\t\t\t\t\tif epoch == epochs-1:\n",
    "\t\t\t\t\t\t\t\tloss_evol[\"loss\"].append(loss_val)\n",
    "\t\t\t\t\t\t\t\tloss_evol[\"dry_loss\"].append(dry_loss_val)\n",
    "\t\t\t\t\t\t\t\tloss_evol[\"entropy\"].append(entrop_val)\n",
    "\n",
    "\t\t\t\t\t\tself.memory.clear_memory()\n",
    "\n",
    "\t\t\t\t\tif done:\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t# Test every 25 episodes\n",
    "\t\t\tif ep == 1 or (ep > 0 and ep % 25 == 0) or (ep == max_episodes - 1):\n",
    "\t\t\t\trewards_test.append(np.array([self.evaluate() for _ in range(50)]))\n",
    "\t\t\t\tprint(f'Episode {ep}/{max_episodes}: Mean rewards: {round(rewards_test[-1].mean(), 2)}, Std: {round(rewards_test[-1].std(), 2)}')\n",
    "\t\t\t\tif round(rewards_test[-1].mean(), 2) == 500.:\n",
    "\t\t\t\t\tsolved = True\n",
    "\n",
    "\t\tself.env.close()\n",
    "\t\tt2 = datetime.datetime.now()\n",
    "\n",
    "\t\t# save rewards\n",
    "\t\tr = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards_test[i]) for i in range(len(rewards_test))))), columns=['Episode', 'Reward'])\n",
    "\t\tr[\"Episode\"] = r[\"Episode\"]*25\n",
    "\t\tr[\"loss_name\"] = self.loss_name\n",
    "\n",
    "\t\t# Total time ellapsed\n",
    "\t\ttime = t2-t1\n",
    "\t\tprint(f'The training was done over a total of {episode_count} episodes')\n",
    "\t\tprint('Total time ellapsed during training : ', time)\n",
    "\t\tr[\"time\"] = time\n",
    "\t\tloss_evol = pd.DataFrame(loss_evol).astype(float)\n",
    "\t\tloss_evol[\"loss_name\"] = self.loss_name\n",
    "\t\tloss_evol[\"Update\"] = range(len(loss_evol))\n",
    "\n",
    "\t\treturn r, loss_evol\n",
    "\n",
    "\tdef compute_proba_ratio(self, prob, actions):\n",
    "\t\tif self.discrete_action_bool:\n",
    "\t\t\t# 1st iteration : initialize old policy to the current one to avoid clipping\n",
    "\t\t\tif len(self.probs_list) == 1:\n",
    "\t\t\t\told_prob = self.probs_list[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\told_prob = self.probs_list[len(self.probs_list)-2]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif len(self.mean_list) == 1:\n",
    "\t\t\t\told_prob_mean = self.mean_list[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\told_prob_mean = self.mean_list[len(self.mean_list)-2]\n",
    "\n",
    "\t\t\t#m = normal.Normal(loc=old_prob_mean.float(), scale=torch.tensor(config[\"std\"]*np.ones(actions.size())).float())\n",
    "\t\t\t#old_prob = m.log_prob(actions.float()).reshape(actions.size()).detach()\n",
    "\t\t\t#print(prob.size())\n",
    "\t\t\t#print(old_prob_mean.size())\n",
    "\n",
    "\t\t\t# build old probabilities \n",
    "\t\t\t#cov_mat = torch.eye(old_prob_mean.size()[1])*self.config[\"std\"]\n",
    "\t\t\t#dist = MultivariateNormal(old_prob_mean, cov_mat)\n",
    "\n",
    "# \t\t\tprint(\"old prob mean\",old_prob_mean.size()[1])\n",
    "# \t\t\tprint(\"old prob mean\",torch.tensor(self.config['std']*np.ones(old_prob_mean.size()[1])).float())\n",
    "\t\t\tdiag = torch.tensor(self.config['std']*np.ones(old_prob_mean.size()[1])).float()\n",
    "# \t\t\tprint(diag)\n",
    "\t\t\tdist = Normal(old_prob_mean, scale = diag)\n",
    "\t\t\t#print(\"hey\")\n",
    "\t\t\t#print(old_prob_mean.size())\n",
    "\t\t\t#print(cov_mat.size())\n",
    "\t\t\t#print(actions.size())\n",
    "\t\t\told_prob = dist.log_prob(actions).detach()\n",
    "\n",
    "\t\t\t# build new ones\n",
    "\t\t\t#print(prob)\n",
    "\t\t\t#dist = MultivariateNormal(prob, cov_mat)\n",
    "\t\t\tdist = Normal(prob, scale = diag)\n",
    "\t\t\tprob = dist.log_prob(actions)\n",
    "\n",
    "\t\tif self.discrete_action_bool:\n",
    "\t\t\t# compute the ratio directly using gather function\n",
    "\t\t\tnum = prob.gather(1, actions.long().view(-1, 1))\n",
    "\t\t\tdenom = old_prob.detach().gather(1, actions.long().view(-1, 1))\n",
    "\t\t\tratio_vect = num.view(-1)/denom.view(-1)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif np.isnan(prob.cpu().detach().numpy()).any():\n",
    "\t\t\t\tprint(\"NaN encountered in num ratio\")\n",
    "\n",
    "\t\t\tif np.isnan(old_prob.cpu().detach().numpy()).any():\n",
    "\t\t\t\tprint(\"NaN encountered in denom ratio\")\n",
    "\n",
    "\t\t\tratio_vect = prob/(old_prob+1e-6)\n",
    "\n",
    "\t\tif np.isnan(ratio_vect.cpu().detach().numpy()).any():\n",
    "\t\t\tprint(\"NaN encountered in proba ratio\")\n",
    "\n",
    "\t\treturn ratio_vect, old_prob\n",
    "\n",
    "\n",
    "\tdef clipped_loss(self, prob, actions, advantages):\n",
    "\n",
    "\t\tratio_vect = self.compute_proba_ratio(prob, actions)[0]\n",
    "\t\t#print(ratio_vect.size())\n",
    "\n",
    "\t\tif len(actions.size()) > 1 and self.discrete_action_bool == False :\n",
    "# \t\t\tprint(\"more than one action\")\n",
    "\t\t\tratio_vect = torch.prod(ratio_vect, dim=1)\n",
    "# \t\t\tprint(\"ratio_vect\",ratio_vect)\n",
    "\n",
    "\t\t# Compute the loss\n",
    "# \t\tprint(\"ratio vect dim\",ratio_vect.size())\n",
    "# \t\tprint(\"adv dim\",advantages.size())\n",
    "\t\tloss1 = ratio_vect * advantages\n",
    "\t\tloss2 = torch.clamp(ratio_vect, 1-self.config['eps_clipping'], 1+self.config['eps_clipping']) * advantages\n",
    "\t\tloss = - torch.sum(torch.min(loss1, loss2))\n",
    "\t\treturn loss\n",
    "\n",
    "\n",
    "\tdef adaptative_KL_loss(self, prob, actions, advantages, observations):\n",
    "\n",
    "\t\tif self.discrete_action_bool:\n",
    "\t\t\tratio_vect, old_prob = self.compute_proba_ratio(prob, actions)\n",
    "\t\t\tkl = torch.zeros(1)\n",
    "\t\t\tfor i in range(prob.size()[0]):\n",
    "\t\t\t\tkl += (old_prob[i] * (old_prob[i].log() - prob[i].log())).sum()\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tratio_vect = self.compute_proba_ratio(prob, actions)[0]\n",
    "\t\t\tif len(self.mean_list) == 1:\n",
    "\t\t\t\tkl = torch.tensor(0.)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmu = prob.view(-1)\n",
    "\t\t\t\tmu_old = self.mean_list[len(self.mean_list)-2].view(-1).detach()\n",
    "\t\t\t\tkl = torch.dot((mu-mu_old)/torch.tensor(config[\"std\"]*np.ones(len(actions))).float(), mu-mu_old)/2\n",
    "\n",
    "\t\tloss = - torch.sum((ratio_vect*advantages)) + self.beta_kl*kl\n",
    "\n",
    "\t\t# Update beta values\n",
    "\t\tif np.isnan(torch.mean(kl).cpu().detach().numpy()):\n",
    "\t\t\tprint(\"Nan encountered in average KL divergence\")\n",
    "\t\tif kl < self.config[\"d_targ\"]/1.5:\n",
    "\t\t\tself.beta_kl = self.beta_kl / 2\n",
    "\t\telif kl > self.config[\"d_targ\"]*1.5:\n",
    "\t\t\tself.beta_kl = self.beta_kl * 2\n",
    "\t\treturn loss\n",
    "\n",
    "\n",
    "\tdef A2C_loss(self, prob, actions, advantages):\n",
    "\t\tloss = 0.\n",
    "\t\tif self.discrete_action_bool:\n",
    "\t\t\tfor i in range(len(actions)):\n",
    "\t\t\t\tloss -= torch.log(prob[i, int(actions[i])]+1e-6)*advantages[i]\n",
    "\t\telse:\n",
    "# \t\t\tprint(prob.size(),\"1\")\n",
    "\t\t\t#cov_mat = torch.eye(prob.size()[1])*self.config[\"std\"]\n",
    "\t\t\t#print(cov_mat.size())\n",
    "\t\t\t#dist = MultivariateNormal(prob, cov_mat)\n",
    "\t\t\tdiag = torch.tensor(self.config[\"std\"]*np.ones(prob.size()[1])).float()\n",
    "# \t\t\tprint(diag.size())\n",
    "\t\t\tdist = Normal(prob, scale = diag)\n",
    "\t\t\tprob = dist.log_prob(actions)\n",
    "# \t\t\tprint(actions.size())\n",
    "# \t\t\tprint(prob.size(),\"2\")\n",
    "\t\t\tif actions.size()[1]>1:\n",
    "\t\t\t\tprob = torch.prod(prob, dim=1)\n",
    "# \t\t\t\tprint(prob.size())\n",
    "\n",
    "\n",
    "\t\t\tloss = torch.dot(torch.log(prob.view(-1)+1e-6), advantages)\n",
    "\n",
    "\t\treturn loss\n",
    "\n",
    "\n",
    "\tdef optimize_model(self, next_obs):\n",
    "# \t\tprint(\"here\")\n",
    "\n",
    "\t\tlosses = {\"loss\": [], \"dry_loss\": [], \"entropy\": []}\n",
    "\t\tidx = torch.arange(len(self.memory))\n",
    "\n",
    "\t\tobservations = torch.tensor(self.memory.observations).float().to(self.device)\n",
    "\t\t# print(observations.size())\n",
    "\t\tif np.isnan(observations.cpu().detach().numpy()).any():\n",
    "\t\t\tprint(\"nan in observations\")\n",
    "\n",
    "\t\tif self.discrete_action_bool:\t\t\n",
    "\t\t\tactions = torch.tensor(self.memory.actions).float().to(self.device)\n",
    "\t\telse:\n",
    "\t\t\tactions = torch.squeeze(torch.stack(self.memory.actions),1).float().to(self.device)\n",
    "# \t\t\tprint(actions.size(), \"bouya\")\n",
    "\n",
    "\n",
    "\t\tnext_obs = torch.from_numpy(next_obs).float().to(self.device)\n",
    "\t\tnext_value = self.value_network.predict(next_obs)\n",
    "\t\tvalues = self.value_network(observations)\n",
    "\t\treturns, advantages = self._returns_advantages(values, next_value)\n",
    "\t\treturns = returns.float().to(self.device)\n",
    "\t\tadvantages = advantages.float().to(self.device)\n",
    "\n",
    "\t\tfor i in range(0, returns.size()[0], self.batch_size):\n",
    "\n",
    "\t\t\tindices = idx[i:i+self.batch_size]\n",
    "\t\t\tbatch_observations = observations[i:i+self.batch_size]\n",
    "\t\t\tbatch_actions = actions[i:i+self.batch_size]\n",
    "\t\t\tbatch_returns = returns[i:i+self.batch_size]\n",
    "\t\t\tbatch_advantages = advantages[i:i+self.batch_size]\n",
    "\n",
    "\t\t\t# Critic loss\n",
    "\t\t\tnet_values: torch.Tensor = self.value_network(batch_observations)\n",
    "\t\t\tcritic_loss = F.mse_loss(net_values.view(-1), batch_returns)\n",
    "\t\t\tcritic_loss.backward()\n",
    "\t\t\tself.value_network_optimizer.step()\n",
    "\n",
    "\t\t\t# Actor & Entropy loss\n",
    "# \t\t\tprint(batch_observations.size())\n",
    "\t\t\tif np.isnan(batch_observations.cpu().detach().numpy()).any():\n",
    "\t\t\t\tprint(\"nan in batch observations\")\n",
    "\n",
    "\n",
    "\t\t\tprob: torch.Tensor = self.actor_network.forward(batch_observations)\n",
    "\t\t\tif np.isnan(prob.cpu().detach().numpy()).any():\n",
    "# \t\t\t\tprint(batch_observations.size(),\"d\")\n",
    "\t\t\t\tprint(\"NAN HERE\")\n",
    "\n",
    "\n",
    "\t\t\tif self.discrete_action_bool:\n",
    "\t\t\t\tself.probs_list.append(prob.detach())\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\t#cov_mat = torch.eye(prob.size()[1])*self.config[\"std\"]\n",
    "\t\t\t\t#dist = MultivariateNormal(prob, cov_mat)\n",
    "\t\t\t\t#logprob = dist.log_prob(batch_actions)\n",
    "\t\t\t\t#print(logprob.size())\n",
    "\t\t\t\t#dist_entropy = dist.entropy()\n",
    "\t\t\t\t#state_value = self.critic(state)\n",
    "\t\t\t\t#m = normal.Normal(loc=prob.float(), scale=torch.tensor(config[\"std\"]*np.ones(actions.size())).float())\n",
    "\t\t\t\t#logprob = m.log_prob(batch_actions.float()).reshape(batch_actions.size())\n",
    "\t\t\t\t#self.probs_list.append(torch.exp(logprob).detach()) # not very useful\n",
    "\t\t\t\t# append the gaussian mean (used to estimate old probability)\n",
    "\t\t\t\tself.mean_list.append(prob.detach())\n",
    "\n",
    "\t\t\tif self.loss_name == \"clipped_loss\":\n",
    "\t\t\t\tloss = self.clipped_loss(prob, batch_actions, batch_advantages)\n",
    "\n",
    "\t\t\telif self.loss_name == \"adaptative_KL_loss\":\n",
    "\t\t\t\tloss = self.adaptative_KL_loss(prob, batch_actions, batch_advantages, batch_observations)\n",
    "\n",
    "\t\t\telif self.loss_name == \"A2C_loss\":\n",
    "\t\t\t\tloss = self.A2C_loss(prob, batch_actions, batch_advantages)\n",
    "\n",
    "\t\t\telse:  # use clipped loss as default\n",
    "\t\t\t\tloss = self.clipped_loss(prob, batch_actions, batch_advantages)\n",
    "\n",
    "\t\t\tdry_loss = loss\n",
    "\t\t\tentropy_term = -torch.sum(prob * torch.log(prob+1e-6))\n",
    "\t\t\t# entropy_term = -torch.sum(prob * torch.log(prob+1e-6), dim=1)\n",
    "\t\t\tloss -= (self.c2 * entropy_term)\n",
    "\t\t\t# loss = loss.sum() - (self.c2 * entropy_term)\n",
    "\t\t\t# loss = loss / n_trajs\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t# loss.sum().backward()\n",
    "\t\t\t# loss.mean().backward()\n",
    "\t\t\tself.actor_network_optimizer.step()\n",
    "\t\t\tself.value_network_optimizer.zero_grad()\n",
    "\t\t\tself.actor_network_optimizer.zero_grad()\n",
    "\n",
    "\t\t\tlosses[\"loss\"].append(loss.mean().item())\n",
    "\t\t\tlosses[\"dry_loss\"].append(dry_loss.mean().item())\n",
    "\t\t\tlosses[\"entropy\"].append(entropy_term.mean().item())\n",
    "\n",
    "\t\treturn np.mean(losses[\"loss\"]), np.mean(losses[\"dry_loss\"]), np.mean(losses[\"entropy\"])\n",
    "\n",
    "\tdef evaluate(self, render=False):\n",
    "\t\tenv = self.monitor_env if render else self.env\n",
    "\t\tobservation = env.reset()\n",
    "\n",
    "\t\tobservation = torch.from_numpy(observation).float().to(self.device)\n",
    "\t\treward_episode = 0\n",
    "\t\tdone = False\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\twhile not done:\n",
    "\t\t\t\tpolicy = self.actor_network(observation)\n",
    "\n",
    "\t\t\t\tif self.discrete_action_bool:\n",
    "\t\t\t\t\taction = int(torch.multinomial(policy, 1))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taction = self.actor_network.select_action(observation)\n",
    "# \t\t\t\tprint(\"evaluate : sampled action : \",action)\n",
    "\t\t\t\tobservation, reward, done, info = env.step(action.view(-1))\n",
    "\t\t\t\tobservation = torch.from_numpy(observation).float().to(self.device)\n",
    "\t\t\t\treward_episode += reward\n",
    "\n",
    "\t\tenv.close()\n",
    "\t\tif render:\n",
    "\t\t\tshow_video(\"./gym-results\")\n",
    "\t\t\tprint(f'Reward: {reward_episode}')\n",
    "\t\treturn reward_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class CustomValueNetwork(nn.Module):\n",
    "\t\"\"\"\n",
    "\tApproximation of the value function V of a state given as input\n",
    "\tFC network with 1 hidden layer and ReLU activations\n",
    "\tClass used as 'critic'\n",
    "\tInputs :\n",
    "\tinput_size : dimension of each state\n",
    "\thidden_size : number of hidden layers\n",
    "\toutput_size : 1 (dimension of the value function estimate)\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, input_size, hidden_size, output_size=1):\n",
    "\t\tsuper(CustomValueNetwork, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(input_size, hidden_size)\n",
    "\t\tself.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\t\tself.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = F.relu(self.fc1(x.float()))\n",
    "\t\tout = F.relu(self.fc2(out))\n",
    "\t\tout = self.fc3(out)\n",
    "\t\treturn out\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\treturn self(x).cpu().detach().numpy()[0]\n",
    "\n",
    "\n",
    "class CustomDiscreteActorNetwork(nn.Module):\n",
    "\t\"\"\"\n",
    "\tCustom policy model network for discrete action space\n",
    "\tInputs :\n",
    "\tinput_size : state space dimension\n",
    "\thidden_size : nb of hidden layers (64 in author's paper for continous action space)\n",
    "\taction_size : action space dimension\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, hidden_size, action_size):\n",
    "\t\tsuper(CustomDiscreteActorNetwork, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(input_size, hidden_size)\n",
    "\t\tself.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\t\tself.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = torch.tanh(self.fc1(x))\n",
    "\t\tout = torch.tanh(self.fc2(out))\n",
    "\t\tout = torch.tanh(self.fc2(out))\n",
    "\t\tout = F.softmax(self.fc3(out), dim=-1)\n",
    "\t\treturn out\n",
    "\n",
    "\tdef select_action(self, x):\n",
    "\t\treturn torch.multinomial(self(x), 1).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "class ContinuousActorNetwork(nn.Module):\n",
    "\t\"\"\"\n",
    "\tPolicy model network for continuous action space (from the paper)\n",
    "\tInputs :\n",
    "\tinput_size : state space dimension\n",
    "\thidden_size : nb of hidden layers used by the authors\n",
    "\taction_size : action space dimension\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, hidden_size, action_size, std, env):\n",
    "\t\tsuper(ContinuousActorNetwork, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(input_size, hidden_size)\n",
    "\t\tself.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\t\tself.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\t\tself.std = std\n",
    "\t\tself.env = env\n",
    "\t\tself.activ = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif np.isnan(x.cpu().detach().numpy()).any():\n",
    "\t\t\tprint(\"nan at the beginning\")\n",
    "# \t\tout = torch.tanh(self.fc1(x.float()))\n",
    "\t\tout = self.activ(self.fc1(x.float()))\n",
    "\t\tif np.isnan(out.cpu().detach().numpy()).any():\n",
    "\t\t\tprint(\"nan after 1\")\n",
    "# \t\tprint(\"weights\",self.fc1.weight)\n",
    "# \t\tprint(\"weights\",self.fc1.weight.size())\n",
    "# \t\tprint(\"min weights\",torch.min(self.fc1.weight))\n",
    "# \t\tprint(\"max weights\",torch.max(self.fc1.weight))\n",
    "# \t\tprint(\"minbias\",torch.min(self.fc1.bias))\n",
    "# \t\tprint(\"maxbias\",torch.max(self.fc1.bias))\n",
    "# \t\tout = torch.tanh(self.fc2(out))\n",
    "\t\tout = self.activ(self.fc2(out))\n",
    "\t\tif np.isnan(out.cpu().detach().numpy()).any():\n",
    "\t\t\tprint(\"nan after 2\")\n",
    "# \t\tout = torch.tanh(self.fc2(out))\n",
    "\t\tout = self.activ(self.fc2(out))\n",
    "\t\tif np.isnan(out.cpu().detach().numpy()).any():\n",
    "\t\t\tprint(\"nan after 2 bis\")\n",
    "# \t\tout = torch.tanh(self.fc3(out))\n",
    "\t\tout = self.activ(self.fc3(out))\n",
    "\t\tif np.isnan(out.cpu().detach().numpy()).any():\n",
    "\t\t\t#print(batch_observations)\n",
    "\t\t\tprint(\"Nan at the end of forward\")\n",
    "\t\t\tprint(\"input tensor x : \",x.float())\n",
    "\t\t\tsys.exit(0)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "\tdef select_action(self, x):\n",
    "\t\taction_mean = self.forward(x)\n",
    "\n",
    "\t\tif np.isnan(action_mean.cpu().detach().numpy()).any():\n",
    "\t\t\t#print(batch_observations)\n",
    "\t\t\tprint(\"Naaaaaaaaan\")\n",
    "\t\tif len(action_mean.size()) == 1 :\n",
    "# \t\t\tprint(\"modify size\")\n",
    "\t\t\taction_mean = action_mean.unsqueeze(0)\n",
    "# \t\t\tprint(action_mean)\n",
    "\t\t#cov_mat = torch.eye(action_mean.size()[1])*self.std\n",
    "\t\t#dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "\t\tdist = Normal(action_mean, scale = torch.tensor(self.std*np.ones(action_mean.size()[1])).float())\n",
    "\t\taction = dist.sample()\n",
    "# \t\tprint(\"sampled action\",action)\n",
    "\t\tif np.isnan(action.cpu().detach().numpy()).any():\n",
    "\t\t\tprint(\"NAAAAAAAAAAAAAN\")\n",
    "\t\t#action_logprob = dist.log_prob(action)\n",
    "\t\t#sampled_a = max(self.env.action_space.low, sampled_a)\n",
    "\t\t#sampled_a = min(self.env.action_space.high, sampled_a)\n",
    "\t \n",
    "\t\treturn action.detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------clipped_loss-----------------\n",
      "{'env': 'BipedalWalker-v3', 'std': 0.5, 'gamma': 0.99, 'lambda': 1, 'lr': 0.0003, 'eps_clipping': 0.2, 'd_targ': 0.01, 'beta_KL': 3, 'c1': 1, 'c2': 0.001, 'reward_norm': False, 'epochs': 1, 'max_episodes': 1000, 'max_steps': 300, 'optimize_every': 128, 'batch_size': 128, 'randomize_batch': False, 'loss_name': 'clipped_loss', 'color': {'A2C_loss': (0.4, 0.7607843137254902, 0.6470588235294118), 'adaptative_KL_loss': (0.9882352941176471, 0.5529411764705883, 0.3843137254901961), 'clipped_loss': (0.5529411764705883, 0.6274509803921569, 0.796078431372549)}, 'seed': 42, 'reset_val': None, 'solved_reward': {'LunarLander-v2': 230, 'MountainCarContinuous-v0': 300, 'CartPole-v1': 300, 'MountainCar-v0': 300}}\n",
      "Low :  [-1. -1. -1. -1.]\n",
      "High :  [1. 1. 1. 1.]\n",
      "Loss :  clipped_loss\n",
      "episode number 0/1000\n",
      "episode number 1/1000\n",
      "Episode 1/1000: Mean rewards: -97.45, Std: 20.98\n",
      "episode number 2/1000\n",
      "episode number 3/1000\n",
      "episode number 4/1000\n",
      "episode number 5/1000\n",
      "episode number 6/1000\n",
      "episode number 7/1000\n",
      "episode number 8/1000\n",
      "episode number 9/1000\n",
      "episode number 10/1000\n",
      "episode number 11/1000\n",
      "episode number 12/1000\n",
      "episode number 13/1000\n",
      "episode number 14/1000\n",
      "episode number 15/1000\n",
      "episode number 16/1000\n",
      "episode number 17/1000\n",
      "episode number 18/1000\n",
      "episode number 19/1000\n",
      "episode number 20/1000\n",
      "episode number 21/1000\n",
      "episode number 22/1000\n",
      "episode number 23/1000\n",
      "episode number 24/1000\n",
      "episode number 25/1000\n",
      "Episode 25/1000: Mean rewards: -90.8, Std: 22.32\n",
      "episode number 26/1000\n",
      "episode number 27/1000\n",
      "episode number 28/1000\n",
      "episode number 29/1000\n",
      "episode number 30/1000\n",
      "episode number 31/1000\n",
      "episode number 32/1000\n",
      "episode number 33/1000\n",
      "episode number 34/1000\n",
      "episode number 35/1000\n",
      "episode number 36/1000\n",
      "episode number 37/1000\n",
      "episode number 38/1000\n",
      "episode number 39/1000\n",
      "episode number 40/1000\n",
      "episode number 41/1000\n",
      "episode number 42/1000\n",
      "episode number 43/1000\n",
      "episode number 44/1000\n",
      "episode number 45/1000\n",
      "episode number 46/1000\n",
      "episode number 47/1000\n",
      "episode number 48/1000\n",
      "episode number 49/1000\n",
      "episode number 50/1000\n",
      "Episode 50/1000: Mean rewards: -94.72, Std: 22.73\n",
      "episode number 51/1000\n",
      "episode number 52/1000\n",
      "episode number 53/1000\n",
      "episode number 54/1000\n",
      "episode number 55/1000\n",
      "episode number 56/1000\n",
      "episode number 57/1000\n",
      "episode number 58/1000\n",
      "episode number 59/1000\n",
      "episode number 60/1000\n",
      "episode number 61/1000\n",
      "episode number 62/1000\n",
      "episode number 63/1000\n",
      "episode number 64/1000\n",
      "episode number 65/1000\n",
      "episode number 66/1000\n",
      "episode number 67/1000\n",
      "episode number 68/1000\n",
      "episode number 69/1000\n",
      "episode number 70/1000\n",
      "episode number 71/1000\n",
      "episode number 72/1000\n",
      "episode number 73/1000\n",
      "episode number 74/1000\n",
      "episode number 75/1000\n",
      "Episode 75/1000: Mean rewards: -91.9, Std: 24.12\n",
      "episode number 76/1000\n",
      "episode number 77/1000\n",
      "episode number 78/1000\n",
      "episode number 79/1000\n",
      "episode number 80/1000\n",
      "episode number 81/1000\n",
      "episode number 82/1000\n",
      "episode number 83/1000\n",
      "episode number 84/1000\n",
      "episode number 85/1000\n",
      "episode number 86/1000\n",
      "episode number 87/1000\n",
      "episode number 88/1000\n",
      "episode number 89/1000\n",
      "episode number 90/1000\n",
      "episode number 91/1000\n",
      "episode number 92/1000\n",
      "episode number 93/1000\n",
      "episode number 94/1000\n",
      "episode number 95/1000\n",
      "episode number 96/1000\n",
      "episode number 97/1000\n",
      "episode number 98/1000\n",
      "episode number 99/1000\n",
      "episode number 100/1000\n",
      "Episode 100/1000: Mean rewards: -90.65, Std: 21.28\n",
      "episode number 101/1000\n",
      "episode number 102/1000\n",
      "episode number 103/1000\n",
      "episode number 104/1000\n",
      "episode number 105/1000\n",
      "episode number 106/1000\n",
      "episode number 107/1000\n",
      "episode number 108/1000\n",
      "episode number 109/1000\n",
      "episode number 110/1000\n",
      "episode number 111/1000\n",
      "episode number 112/1000\n",
      "episode number 113/1000\n",
      "episode number 114/1000\n",
      "episode number 115/1000\n",
      "episode number 116/1000\n",
      "episode number 117/1000\n",
      "episode number 118/1000\n",
      "episode number 119/1000\n",
      "episode number 120/1000\n",
      "episode number 121/1000\n",
      "episode number 122/1000\n",
      "episode number 123/1000\n",
      "episode number 124/1000\n",
      "episode number 125/1000\n",
      "Episode 125/1000: Mean rewards: -90.93, Std: 23.06\n",
      "episode number 126/1000\n",
      "episode number 127/1000\n",
      "episode number 128/1000\n",
      "episode number 129/1000\n",
      "episode number 130/1000\n",
      "episode number 131/1000\n",
      "episode number 132/1000\n",
      "episode number 133/1000\n",
      "episode number 134/1000\n",
      "episode number 135/1000\n",
      "episode number 136/1000\n",
      "episode number 137/1000\n",
      "episode number 138/1000\n",
      "episode number 139/1000\n",
      "episode number 140/1000\n",
      "episode number 141/1000\n",
      "episode number 142/1000\n",
      "episode number 143/1000\n",
      "episode number 144/1000\n",
      "episode number 145/1000\n",
      "episode number 146/1000\n",
      "episode number 147/1000\n",
      "episode number 148/1000\n",
      "episode number 149/1000\n",
      "episode number 150/1000\n",
      "Episode 150/1000: Mean rewards: -92.95, Std: 23.86\n",
      "episode number 151/1000\n",
      "episode number 152/1000\n",
      "episode number 153/1000\n",
      "episode number 154/1000\n",
      "episode number 155/1000\n",
      "episode number 156/1000\n",
      "episode number 157/1000\n",
      "episode number 158/1000\n",
      "episode number 159/1000\n",
      "episode number 160/1000\n",
      "episode number 161/1000\n",
      "episode number 162/1000\n",
      "episode number 163/1000\n",
      "episode number 164/1000\n",
      "episode number 165/1000\n",
      "episode number 166/1000\n",
      "episode number 167/1000\n",
      "episode number 168/1000\n",
      "episode number 169/1000\n",
      "episode number 170/1000\n",
      "episode number 171/1000\n",
      "episode number 172/1000\n",
      "episode number 173/1000\n",
      "episode number 174/1000\n",
      "episode number 175/1000\n",
      "Episode 175/1000: Mean rewards: -87.46, Std: 21.65\n",
      "episode number 176/1000\n",
      "episode number 177/1000\n",
      "episode number 178/1000\n",
      "episode number 179/1000\n",
      "episode number 180/1000\n",
      "episode number 181/1000\n",
      "episode number 182/1000\n",
      "episode number 183/1000\n",
      "episode number 184/1000\n",
      "episode number 185/1000\n",
      "episode number 186/1000\n",
      "episode number 187/1000\n",
      "episode number 188/1000\n",
      "episode number 189/1000\n",
      "episode number 190/1000\n",
      "episode number 191/1000\n",
      "episode number 192/1000\n",
      "episode number 193/1000\n",
      "episode number 194/1000\n",
      "episode number 195/1000\n",
      "episode number 196/1000\n",
      "episode number 197/1000\n",
      "episode number 198/1000\n",
      "episode number 199/1000\n",
      "episode number 200/1000\n",
      "Episode 200/1000: Mean rewards: -88.27, Std: 23.51\n",
      "episode number 201/1000\n",
      "episode number 202/1000\n",
      "episode number 203/1000\n",
      "episode number 204/1000\n",
      "episode number 205/1000\n",
      "episode number 206/1000\n",
      "episode number 207/1000\n",
      "episode number 208/1000\n",
      "episode number 209/1000\n",
      "episode number 210/1000\n",
      "episode number 211/1000\n",
      "episode number 212/1000\n",
      "episode number 213/1000\n",
      "episode number 214/1000\n",
      "episode number 215/1000\n",
      "episode number 216/1000\n",
      "episode number 217/1000\n",
      "episode number 218/1000\n",
      "episode number 219/1000\n",
      "episode number 220/1000\n",
      "episode number 221/1000\n",
      "episode number 222/1000\n",
      "episode number 223/1000\n",
      "episode number 224/1000\n",
      "episode number 225/1000\n",
      "Episode 225/1000: Mean rewards: -87.22, Std: 22.91\n",
      "episode number 226/1000\n",
      "episode number 227/1000\n",
      "episode number 228/1000\n",
      "episode number 229/1000\n",
      "episode number 230/1000\n",
      "episode number 231/1000\n",
      "episode number 232/1000\n",
      "episode number 233/1000\n",
      "episode number 234/1000\n",
      "episode number 235/1000\n",
      "episode number 236/1000\n",
      "episode number 237/1000\n",
      "episode number 238/1000\n",
      "episode number 239/1000\n",
      "episode number 240/1000\n",
      "episode number 241/1000\n",
      "episode number 242/1000\n",
      "episode number 243/1000\n",
      "episode number 244/1000\n",
      "episode number 245/1000\n",
      "episode number 246/1000\n",
      "episode number 247/1000\n",
      "episode number 248/1000\n",
      "episode number 249/1000\n",
      "episode number 250/1000\n",
      "Episode 250/1000: Mean rewards: -85.17, Std: 24.24\n",
      "episode number 251/1000\n",
      "episode number 252/1000\n",
      "episode number 253/1000\n",
      "episode number 254/1000\n",
      "episode number 255/1000\n",
      "episode number 256/1000\n",
      "episode number 257/1000\n",
      "episode number 258/1000\n",
      "episode number 259/1000\n",
      "episode number 260/1000\n",
      "episode number 261/1000\n",
      "episode number 262/1000\n",
      "episode number 263/1000\n",
      "episode number 264/1000\n",
      "episode number 265/1000\n",
      "episode number 266/1000\n",
      "episode number 267/1000\n",
      "episode number 268/1000\n",
      "episode number 269/1000\n",
      "episode number 270/1000\n",
      "episode number 271/1000\n",
      "episode number 272/1000\n",
      "episode number 273/1000\n",
      "episode number 274/1000\n",
      "episode number 275/1000\n",
      "Episode 275/1000: Mean rewards: -91.58, Std: 22.52\n",
      "episode number 276/1000\n",
      "episode number 277/1000\n",
      "episode number 278/1000\n",
      "episode number 279/1000\n",
      "episode number 280/1000\n",
      "episode number 281/1000\n",
      "episode number 282/1000\n",
      "episode number 283/1000\n",
      "episode number 284/1000\n",
      "episode number 285/1000\n",
      "episode number 286/1000\n",
      "episode number 287/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number 288/1000\n",
      "episode number 289/1000\n",
      "episode number 290/1000\n",
      "episode number 291/1000\n",
      "episode number 292/1000\n",
      "episode number 293/1000\n",
      "episode number 294/1000\n",
      "episode number 295/1000\n",
      "episode number 296/1000\n",
      "episode number 297/1000\n",
      "episode number 298/1000\n",
      "episode number 299/1000\n",
      "episode number 300/1000\n",
      "Episode 300/1000: Mean rewards: -87.01, Std: 23.18\n",
      "episode number 301/1000\n",
      "episode number 302/1000\n",
      "episode number 303/1000\n",
      "episode number 304/1000\n",
      "episode number 305/1000\n",
      "episode number 306/1000\n",
      "episode number 307/1000\n",
      "episode number 308/1000\n",
      "episode number 309/1000\n",
      "episode number 310/1000\n",
      "episode number 311/1000\n",
      "episode number 312/1000\n",
      "episode number 313/1000\n",
      "episode number 314/1000\n",
      "episode number 315/1000\n",
      "episode number 316/1000\n",
      "episode number 317/1000\n",
      "episode number 318/1000\n",
      "episode number 319/1000\n",
      "episode number 320/1000\n",
      "episode number 321/1000\n",
      "episode number 322/1000\n",
      "episode number 323/1000\n",
      "episode number 324/1000\n",
      "episode number 325/1000\n",
      "Episode 325/1000: Mean rewards: -87.07, Std: 24.07\n",
      "episode number 326/1000\n",
      "episode number 327/1000\n",
      "episode number 328/1000\n",
      "episode number 329/1000\n",
      "episode number 330/1000\n",
      "episode number 331/1000\n",
      "episode number 332/1000\n",
      "episode number 333/1000\n",
      "episode number 334/1000\n",
      "episode number 335/1000\n",
      "episode number 336/1000\n",
      "episode number 337/1000\n",
      "episode number 338/1000\n",
      "episode number 339/1000\n",
      "episode number 340/1000\n",
      "episode number 341/1000\n",
      "episode number 342/1000\n",
      "episode number 343/1000\n",
      "episode number 344/1000\n",
      "episode number 345/1000\n",
      "episode number 346/1000\n",
      "episode number 347/1000\n",
      "episode number 348/1000\n",
      "episode number 349/1000\n",
      "episode number 350/1000\n",
      "Episode 350/1000: Mean rewards: -89.17, Std: 22.99\n",
      "episode number 351/1000\n",
      "episode number 352/1000\n",
      "episode number 353/1000\n",
      "episode number 354/1000\n",
      "episode number 355/1000\n",
      "episode number 356/1000\n",
      "episode number 357/1000\n",
      "episode number 358/1000\n",
      "episode number 359/1000\n",
      "episode number 360/1000\n",
      "episode number 361/1000\n",
      "episode number 362/1000\n",
      "episode number 363/1000\n",
      "episode number 364/1000\n",
      "episode number 365/1000\n",
      "episode number 366/1000\n",
      "episode number 367/1000\n",
      "episode number 368/1000\n",
      "episode number 369/1000\n",
      "episode number 370/1000\n",
      "episode number 371/1000\n",
      "episode number 372/1000\n",
      "episode number 373/1000\n",
      "episode number 374/1000\n",
      "episode number 375/1000\n",
      "Episode 375/1000: Mean rewards: -91.51, Std: 23.7\n",
      "episode number 376/1000\n",
      "episode number 377/1000\n",
      "episode number 378/1000\n",
      "episode number 379/1000\n",
      "episode number 380/1000\n",
      "episode number 381/1000\n",
      "episode number 382/1000\n",
      "episode number 383/1000\n",
      "episode number 384/1000\n",
      "episode number 385/1000\n",
      "episode number 386/1000\n",
      "episode number 387/1000\n",
      "episode number 388/1000\n",
      "episode number 389/1000\n",
      "episode number 390/1000\n",
      "episode number 391/1000\n",
      "episode number 392/1000\n",
      "episode number 393/1000\n",
      "episode number 394/1000\n",
      "episode number 395/1000\n",
      "episode number 396/1000\n",
      "episode number 397/1000\n",
      "episode number 398/1000\n",
      "episode number 399/1000\n",
      "episode number 400/1000\n",
      "Episode 400/1000: Mean rewards: -95.04, Std: 20.94\n",
      "episode number 401/1000\n",
      "episode number 402/1000\n",
      "episode number 403/1000\n",
      "episode number 404/1000\n",
      "episode number 405/1000\n",
      "episode number 406/1000\n",
      "episode number 407/1000\n",
      "episode number 408/1000\n",
      "episode number 409/1000\n",
      "episode number 410/1000\n",
      "episode number 411/1000\n",
      "episode number 412/1000\n",
      "episode number 413/1000\n",
      "episode number 414/1000\n",
      "episode number 415/1000\n",
      "episode number 416/1000\n",
      "episode number 417/1000\n",
      "episode number 418/1000\n",
      "episode number 419/1000\n",
      "episode number 420/1000\n",
      "episode number 421/1000\n",
      "episode number 422/1000\n",
      "episode number 423/1000\n",
      "episode number 424/1000\n",
      "episode number 425/1000\n",
      "Episode 425/1000: Mean rewards: -89.0, Std: 23.43\n",
      "episode number 426/1000\n",
      "episode number 427/1000\n",
      "episode number 428/1000\n",
      "episode number 429/1000\n",
      "episode number 430/1000\n",
      "episode number 431/1000\n",
      "episode number 432/1000\n",
      "episode number 433/1000\n",
      "episode number 434/1000\n",
      "episode number 435/1000\n",
      "episode number 436/1000\n",
      "episode number 437/1000\n",
      "episode number 438/1000\n",
      "episode number 439/1000\n",
      "episode number 440/1000\n",
      "episode number 441/1000\n",
      "episode number 442/1000\n",
      "episode number 443/1000\n",
      "episode number 444/1000\n",
      "episode number 445/1000\n",
      "episode number 446/1000\n",
      "episode number 447/1000\n",
      "episode number 448/1000\n",
      "episode number 449/1000\n",
      "episode number 450/1000\n",
      "Episode 450/1000: Mean rewards: -84.95, Std: 21.95\n",
      "episode number 451/1000\n",
      "episode number 452/1000\n",
      "episode number 453/1000\n",
      "episode number 454/1000\n",
      "episode number 455/1000\n",
      "episode number 456/1000\n",
      "episode number 457/1000\n",
      "episode number 458/1000\n",
      "episode number 459/1000\n",
      "episode number 460/1000\n",
      "episode number 461/1000\n",
      "episode number 462/1000\n",
      "episode number 463/1000\n",
      "episode number 464/1000\n",
      "episode number 465/1000\n",
      "episode number 466/1000\n",
      "episode number 467/1000\n",
      "episode number 468/1000\n",
      "episode number 469/1000\n",
      "episode number 470/1000\n",
      "episode number 471/1000\n",
      "episode number 472/1000\n",
      "episode number 473/1000\n",
      "episode number 474/1000\n",
      "episode number 475/1000\n",
      "Episode 475/1000: Mean rewards: -91.38, Std: 21.96\n",
      "episode number 476/1000\n",
      "episode number 477/1000\n",
      "episode number 478/1000\n",
      "episode number 479/1000\n",
      "episode number 480/1000\n",
      "episode number 481/1000\n",
      "episode number 482/1000\n",
      "episode number 483/1000\n",
      "episode number 484/1000\n",
      "episode number 485/1000\n",
      "episode number 486/1000\n",
      "episode number 487/1000\n",
      "episode number 488/1000\n",
      "episode number 489/1000\n",
      "episode number 490/1000\n",
      "episode number 491/1000\n",
      "episode number 492/1000\n",
      "episode number 493/1000\n",
      "episode number 494/1000\n",
      "episode number 495/1000\n",
      "episode number 496/1000\n",
      "episode number 497/1000\n",
      "episode number 498/1000\n",
      "episode number 499/1000\n",
      "episode number 500/1000\n",
      "Episode 500/1000: Mean rewards: -86.52, Std: 22.46\n",
      "episode number 501/1000\n",
      "episode number 502/1000\n",
      "episode number 503/1000\n",
      "episode number 504/1000\n",
      "episode number 505/1000\n",
      "episode number 506/1000\n",
      "episode number 507/1000\n",
      "episode number 508/1000\n",
      "episode number 509/1000\n",
      "episode number 510/1000\n",
      "episode number 511/1000\n",
      "episode number 512/1000\n",
      "episode number 513/1000\n",
      "episode number 514/1000\n",
      "episode number 515/1000\n",
      "episode number 516/1000\n",
      "episode number 517/1000\n",
      "episode number 518/1000\n",
      "episode number 519/1000\n",
      "episode number 520/1000\n",
      "episode number 521/1000\n",
      "episode number 522/1000\n",
      "episode number 523/1000\n",
      "episode number 524/1000\n",
      "episode number 525/1000\n",
      "Episode 525/1000: Mean rewards: -92.75, Std: 20.65\n",
      "episode number 526/1000\n",
      "episode number 527/1000\n",
      "episode number 528/1000\n",
      "episode number 529/1000\n",
      "episode number 530/1000\n",
      "episode number 531/1000\n",
      "episode number 532/1000\n",
      "episode number 533/1000\n",
      "episode number 534/1000\n",
      "episode number 535/1000\n",
      "episode number 536/1000\n",
      "episode number 537/1000\n",
      "episode number 538/1000\n",
      "episode number 539/1000\n",
      "episode number 540/1000\n",
      "episode number 541/1000\n",
      "episode number 542/1000\n",
      "episode number 543/1000\n",
      "episode number 544/1000\n",
      "episode number 545/1000\n",
      "episode number 546/1000\n",
      "episode number 547/1000\n",
      "episode number 548/1000\n",
      "episode number 549/1000\n",
      "episode number 550/1000\n",
      "Episode 550/1000: Mean rewards: -91.07, Std: 21.43\n",
      "episode number 551/1000\n",
      "episode number 552/1000\n",
      "episode number 553/1000\n",
      "episode number 554/1000\n",
      "episode number 555/1000\n",
      "episode number 556/1000\n",
      "episode number 557/1000\n",
      "episode number 558/1000\n",
      "episode number 559/1000\n",
      "episode number 560/1000\n",
      "episode number 561/1000\n",
      "episode number 562/1000\n",
      "episode number 563/1000\n",
      "episode number 564/1000\n",
      "episode number 565/1000\n",
      "episode number 566/1000\n",
      "episode number 567/1000\n",
      "episode number 568/1000\n",
      "episode number 569/1000\n",
      "episode number 570/1000\n",
      "episode number 571/1000\n",
      "episode number 572/1000\n",
      "episode number 573/1000\n",
      "episode number 574/1000\n",
      "episode number 575/1000\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "loss_list = []\n",
    "config[\"epochs\"]=1\n",
    "for loss in [\"clipped_loss\",\"adaptative_KL_loss\",\"A2C_loss\"]:\n",
    "# for loss in [\"A2C_loss\"]:\n",
    "    print(\"-----------------\"+loss+\"-----------------\")\n",
    "    config[\"loss_name\"]=loss\n",
    "    print(config)\n",
    "    agent = PPOAgent(config)\n",
    "    \n",
    "    rewards, loss = agent.training(config[\"epochs\"], config[\"optimize_every\"], config[\"max_episodes\"], config[\"max_steps\"])\n",
    "    rewards_list.append(rewards)\n",
    "    loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_sumup(rewards_list,loss_list,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PPO_A2C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
