{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n",
      "Device name:  GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import gym\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.distributions import normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using GPU\" if device.type==\"cuda\" else \"using CPU\" )\n",
    "print(\"Device name: \", torch.cuda.get_device_name())\n",
    "\n",
    "from ppo import PPO\n",
    "from memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_config(print_, env_name):\n",
    "    config = {}\n",
    "    \n",
    "    config['env'] = env_name\n",
    "\n",
    "    config['std'] = 0.001     \n",
    "    config['gamma'] = 0.99     \n",
    "    config['lambda'] = 0.95    \n",
    "    config['critic'] = {'lr': 0.002, \"hidden\":64}\n",
    "    config['actor'] = {'lr': 0.002, \"hidden\":64}\n",
    "    config['eps_clipping'] = 0.2 \n",
    "    config['c1'] = 0.5      #parameter of the value function loss\n",
    "    config['c2'] = 0.01  #entropy parameter --> 1e-4 to 1e-2 \n",
    "    \n",
    "    config['epochs'] = 4\n",
    "    config['max_episodes'] = 1000\n",
    "    config['max_steps'] = 300\n",
    "    config['optimize_every'] = 2000\n",
    "    config['batch_size'] = 128\n",
    "\n",
    "    config[\"solved_reward\"] = {'LunarLander-v2':230,\n",
    "                              'MountainCarContinuous-v0':300,\n",
    "                              'CartPole-v1':300,\n",
    "                              'MountainCar-v0':300}\n",
    "    \n",
    "    config['seed'] = 42\n",
    "    \n",
    "    if print_== True :\n",
    "        print(\"Training config : \\n\")\n",
    "        pprint(config)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config : \n",
      "\n",
      "{'actor': {'hidden': 64, 'lr': 0.002},\n",
      " 'batch_size': 128,\n",
      " 'c1': 0.5,\n",
      " 'c2': 0.01,\n",
      " 'critic': {'hidden': 64, 'lr': 0.002},\n",
      " 'env': 'LunarLander-v2',\n",
      " 'epochs': 4,\n",
      " 'eps_clipping': 0.2,\n",
      " 'gamma': 0.99,\n",
      " 'lambda': 0.95,\n",
      " 'max_episodes': 1000,\n",
      " 'max_steps': 300,\n",
      " 'optimize_every': 2000,\n",
      " 'seed': 42,\n",
      " 'solved_reward': {'CartPole-v1': 300,\n",
      "                   'LunarLander-v2': 230,\n",
      "                   'MountainCar-v0': 300,\n",
      "                   'MountainCarContinuous-v0': 300},\n",
      " 'std': 0.001}\n"
     ]
    }
   ],
   "source": [
    "envs = ['MountainCar-v0','CartPole-v1','MountainCarContinuous-v0','LunarLander-v2']\n",
    "config = reset_config(True, envs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(config):\n",
    "    \n",
    "    t1 = datetime.datetime.now()\n",
    "    episode_count = 0\n",
    "    timestep_count = 0\n",
    "    loss_evol = {'loss':[],'dry_loss':[],'entropy':[]}\n",
    "    rewards_test = []\n",
    "    \n",
    "    env = gym.make(config['env'])\n",
    "    memory = Memory()\n",
    "    ppo = PPO(config, env, device)\n",
    "\n",
    "    for ep in range(config[\"max_episodes\"]):\n",
    "        episode_count +=1\n",
    "        obs = env.reset()\n",
    "\n",
    "        for i in range(config[\"max_steps\"]):\n",
    "            timestep_count +=1 \n",
    "\n",
    "            memory.observations.append(obs)\n",
    "            obs_t = torch.from_numpy(obs).float().to(device)  \n",
    "            action = ppo.actorcritic.select_action(obs_t, memory)  \n",
    "            memory.values.append(ppo.actorcritic.predict(obs_t))\n",
    "\n",
    "            action = int(action)\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action) \n",
    "            memory.dones.append(done) \n",
    "            memory.rewards.append(reward) \n",
    "\n",
    "            if (timestep_count % config[\"optimize_every\"]) == 0:\n",
    "                loss_val, dry_loss_val, entrop_val = ppo.update(memory,next_obs)\n",
    "                \n",
    "                loss_evol[\"loss\"].append(loss_val)\n",
    "                loss_evol[\"dry_loss\"].append(dry_loss_val)\n",
    "                loss_evol[\"entropy\"].append(entrop_val)\n",
    "                \n",
    "                memory.clear_memory()\n",
    "\n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        if ep == 1 or (ep > 0 and ep % 25 == 0) or (ep == config[\"max_episodes\"] - 1):\n",
    "            rewards_test.append(np.array([ppo.test() for _ in range(50)]))\n",
    "            print(f'Episode {ep}/{config[\"max_episodes\"]}: Mean rewards: {round(rewards_test[-1].mean(), 2)}, Std: {round(rewards_test[-1].std(), 2)}')\n",
    "\n",
    "    env.close()\n",
    "    t2 = datetime.datetime.now()\n",
    "        \n",
    "        # save rewards\n",
    "        #r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards_test[i]) for i in range(len(rewards_test))))), columns=['Episode', 'Reward'])\n",
    "        #r[\"Episode\"] = r[\"Episode\"]*25\n",
    "        #r[\"loss_name\"] = self.loss_name # add loss name as label\n",
    "        \n",
    "        # Plot\n",
    "        #sns.lineplot(x=\"Episode\", y=\"Reward\", data=r, ci='sd',color=config[\"color\"][self.loss_name],label=self.loss_name)\n",
    "        # Total time ellapsed\n",
    "        #time = t2-t1\n",
    "        #print(f'The training was done over a total of {episode_count} episodes')\n",
    "        #print('Total time ellapsed during training : ',time)\n",
    "        #r[\"time\"]=time\n",
    "        #loss_evol = pd.DataFrame(loss_evol).astype(float)\n",
    "        #loss_evol[\"loss_name\"] = self.loss_name\n",
    "        #loss_evol[\"Update\"] = range(len(loss_evol))\n",
    "        #return r, loss_evol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitri/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000: Mean rewards: -212.41, Std: 138.1\n",
      "Episode 25/1000: Mean rewards: -180.2, Std: 95.6\n",
      "Episode 50/1000: Mean rewards: -191.19, Std: 112.25\n",
      "Episode 75/1000: Mean rewards: -167.94, Std: 107.73\n",
      "Episode 100/1000: Mean rewards: -145.55, Std: 69.52\n",
      "Episode 125/1000: Mean rewards: -165.61, Std: 99.48\n",
      "Episode 150/1000: Mean rewards: -150.54, Std: 83.47\n",
      "Episode 175/1000: Mean rewards: -183.08, Std: 108.74\n",
      "Episode 200/1000: Mean rewards: -177.79, Std: 111.86\n",
      "Episode 225/1000: Mean rewards: -183.57, Std: 122.88\n",
      "Episode 250/1000: Mean rewards: -189.48, Std: 117.33\n",
      "Episode 275/1000: Mean rewards: -242.24, Std: 132.0\n",
      "Episode 300/1000: Mean rewards: -160.58, Std: 114.31\n",
      "Episode 325/1000: Mean rewards: -188.03, Std: 116.53\n",
      "Episode 350/1000: Mean rewards: -256.29, Std: 156.71\n",
      "Episode 375/1000: Mean rewards: -198.44, Std: 130.31\n",
      "Episode 400/1000: Mean rewards: -158.5, Std: 95.25\n",
      "Episode 425/1000: Mean rewards: -172.74, Std: 129.49\n",
      "Episode 450/1000: Mean rewards: -200.5, Std: 129.54\n",
      "Episode 475/1000: Mean rewards: -255.34, Std: 149.12\n",
      "Episode 500/1000: Mean rewards: -227.67, Std: 147.6\n",
      "Episode 525/1000: Mean rewards: -219.46, Std: 145.81\n",
      "Episode 550/1000: Mean rewards: -262.15, Std: 177.14\n",
      "Episode 575/1000: Mean rewards: -201.94, Std: 149.86\n",
      "Episode 600/1000: Mean rewards: -218.53, Std: 127.31\n",
      "Episode 625/1000: Mean rewards: -197.09, Std: 116.42\n",
      "Episode 650/1000: Mean rewards: -213.66, Std: 134.71\n",
      "Episode 675/1000: Mean rewards: -150.62, Std: 119.1\n",
      "Episode 700/1000: Mean rewards: -157.85, Std: 120.32\n",
      "Episode 725/1000: Mean rewards: -216.79, Std: 116.79\n",
      "Episode 750/1000: Mean rewards: -252.42, Std: 154.64\n",
      "Episode 775/1000: Mean rewards: -200.32, Std: 137.98\n",
      "Episode 800/1000: Mean rewards: -164.62, Std: 118.15\n",
      "Episode 825/1000: Mean rewards: -199.97, Std: 112.36\n",
      "Episode 850/1000: Mean rewards: -187.04, Std: 128.39\n",
      "Episode 875/1000: Mean rewards: -230.52, Std: 133.62\n",
      "Episode 900/1000: Mean rewards: -283.9, Std: 136.66\n",
      "Episode 925/1000: Mean rewards: -213.46, Std: 162.81\n",
      "Episode 950/1000: Mean rewards: -177.56, Std: 137.82\n",
      "Episode 975/1000: Mean rewards: -172.73, Std: 113.26\n",
      "Episode 999/1000: Mean rewards: -216.41, Std: 120.66\n"
     ]
    }
   ],
   "source": [
    "training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
