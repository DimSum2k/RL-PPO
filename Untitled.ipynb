{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from networks import CustomDiscreteActorNetwork\n",
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clipped_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-16 19:13:03,474] Making new env: CartPole-v1\n",
      "C:\\Users\\delan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Score :500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "policy = CustomDiscreteActorNetwork(env.observation_space.shape[0],\n",
    "                                    64,\n",
    "                                    env.action_space.n)\n",
    "\n",
    "path = \"experiences\\CartPole-v1_21806942\\weights\\clipped_loss_actor.pth\"\n",
    "\n",
    "policy.load_state_dict(torch.load(path))\n",
    "policy.eval()\n",
    "\n",
    "for i_episode in range(1):\n",
    "\n",
    "    observation = env.reset()  # reset for each new trial\n",
    "    score = 0\n",
    "    for t in range(1000):  # run for 100 timesteps or until done, whichever is first\n",
    "        env.render()\n",
    "        observation = torch.from_numpy(observation).float()\n",
    "        action = policy.select_action(observation)[0]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if done or t == 999:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(\"Score :\" + str(score))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C_loss_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-16 19:13:12,062] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Score :500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "policy = CustomDiscreteActorNetwork(env.observation_space.shape[0],\n",
    "                                    64,\n",
    "                                    env.action_space.n)\n",
    "\n",
    "path = \"experiences\\CartPole-v1_21806942\\weights\\A2C_loss_actor.pth\"\n",
    "\n",
    "policy.load_state_dict(torch.load(path))\n",
    "policy.eval()\n",
    "\n",
    "for i_episode in range(1):\n",
    "\n",
    "    observation = env.reset()  # reset for each new trial\n",
    "    score = 0\n",
    "    for t in range(1000):  # run for 100 timesteps or until done, whichever is first\n",
    "        env.render()\n",
    "        observation = torch.from_numpy(observation).float()\n",
    "        action = policy.select_action(observation)[0]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if done or t == 999:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(\"Score :\" + str(score))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adaptative_KL_loss_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-16 19:13:20,411] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 53 timesteps\n",
      "Score :53.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "policy = CustomDiscreteActorNetwork(env.observation_space.shape[0],\n",
    "                                    64,\n",
    "                                    env.action_space.n)\n",
    "\n",
    "path = \"experiences\\CartPole-v1_2199621\\weights\\c.pth\"\n",
    "\n",
    "policy.load_state_dict(torch.load(path))\n",
    "policy.eval()\n",
    "\n",
    "for i_episode in range(1):\n",
    "\n",
    "    observation = env.reset()  # reset for each new trial\n",
    "    score = 0\n",
    "    for t in range(1000):  # run for 100 timesteps or until done, whichever is first\n",
    "        env.render()\n",
    "        observation = torch.from_numpy(observation).float()\n",
    "        action = policy.select_action(observation)[0]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if done or t == 999:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(\"Score :\" + str(score))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.3.2 in c:\\users\\delan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: future in c:\\users\\delan\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyglet==1.3.2) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyglet==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-05-16 19:16:00,578] Making new env: CartPole-v1\n",
      "C:\\Users\\delan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Score :553.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import gym \n",
    "\n",
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "#Make gym env\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "policy = CustomDiscreteActorNetwork(env.observation_space.shape[0],\n",
    "                                    64,\n",
    "                                    env.action_space.n)\n",
    "\n",
    "path = \"experiences\\CartPole-v1_21806942\\weights\\A2C_loss_actor.pth\"\n",
    "\n",
    "policy.load_state_dict(torch.load(path))\n",
    "policy.eval()\n",
    "\n",
    "#Run the env\n",
    "observation = env.reset()\n",
    "frames = []\n",
    "for t in range(1000):\n",
    "    #Render to frames buffer\n",
    "    frames.append(env.render(mode=\"rgb_array\"))\n",
    "    observation = torch.from_numpy(observation).float()\n",
    "    action = policy.select_action(observation)[0]\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "\n",
    "    if done or t == 999:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        print(\"Score :\" + str(score))\n",
    "        break\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
