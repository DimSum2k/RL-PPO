{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from pprint import pprint\n",
    "# from pyvirtualdisplay import Display\n",
    "# from IPython import display as ipythondisplay\n",
    "# from IPython.display import clear_output\n",
    "# import base64\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "See https://github.com/openai/gym/wiki/Leaderboard for description of OpenAIGym environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete control settings : \n",
    "* \"Taxi-v3\" : episodic & discrete setting both for states and actions (S/N/E/W/Pickup/Dropoff)\n",
    "* __\"CartPole-v0\" / \"CartPole-v1\" : discrete action and continuous state space__\n",
    "* __\"MountainCar-v0\" : discrete action and continuous state space__\n",
    "\n",
    "Continous control settings :\n",
    "* __\"MountainCarContinuous-v0\" (both state and action continuous spaces)__\n",
    "* \"LunarLanderContinuous-v2\" (requires box2D)\n",
    "* \"CarRacing-v0\" : \"Discrete control is reasonable in this environment as well\" (requires box2D)\n",
    "\n",
    "* \"MsPacman-v0 \"(requires the Atari dependency)\n",
    "\n",
    "Settings used by the authors (requires the MuJoCo dependencies, 30 days free trial) : https://github.com/openai/mujoco-py\n",
    "* \"HalfCheetah-v1\" (use v2 now?)\n",
    "* \"Hopper-v1\"\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config : \n",
      " {'env': 'MountainCar-v0', 'gamma': 0.99, 'value_network': {'lr': 0.001}, 'actor_network': {'lr': 0.001}, 'seed': 1}\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "# config['env'] = 'CartPole-v1'\n",
    "# config['env'] = 'MountainCarContinuous-v0'\n",
    "config['env'] = 'MountainCar-v0'\n",
    "\n",
    "config['gamma'] = 0.99 #Discount rate\n",
    "config['value_network'] = {'lr': 1e-3}\n",
    "config['actor_network'] = {'lr': 1e-3}\n",
    "# config['eps_clipping'] = 0.2\n",
    "# config['d_targ'] = 0.01\n",
    "# config['beta_KL'] = 3\n",
    "# config['entropy'] = 1e-3\n",
    "# config['epochs'] = 10\n",
    "# config['batch_size'] = 10\n",
    "\n",
    "config['seed'] = 1\n",
    "\n",
    "print(\"Training config : \\n\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space : Box(2,)\n",
      "continuous state space\n",
      "Lower bound :  [-1.2  -0.07]\n",
      "Upper bound :  [0.6  0.07]\n",
      "\n",
      "Action Space : Discrete(3)\n",
      "discrete action space\n",
      "\n",
      "Reward range (-inf, inf)\n",
      "\n",
      "Initial state :  [-0.55824552  0.        ]\n",
      "Sampled action :  1\n",
      "\n",
      "Episode over :  False\n"
     ]
    }
   ],
   "source": [
    "## Define environment \n",
    "env = gym.make(config['env'])\n",
    "\n",
    "## Save episode\n",
    "# env = Monitor(env, \"./gym-results\", force=True)\n",
    "\n",
    "## Action and state spaces\n",
    "print(\"State Space : {}\".format(env.observation_space))\n",
    "if 'is_bounded' in dir(env.observation_space):\n",
    "    print(\"continuous state space\") \n",
    "    print(\"Lower bound : \", env.observation_space.low)\n",
    "    print(\"Upper bound : \", env.observation_space.high)\n",
    "else :\n",
    "    print(\"discrete state space\")\n",
    "\n",
    "print(\"\\nAction Space : {}\".format(env.action_space))\n",
    "if 'is_bounded' in dir(env.action_space):\n",
    "    print(\"continuous action space\") \n",
    "    print(\"Lower bound : \", env.action_space.low)\n",
    "    print(\"Uppder bound : \", env.action_space.high)\n",
    "else :\n",
    "    print(\"discrete action space\")\n",
    "\n",
    "print(\"\\nReward range\", env.reward_range)\n",
    "    \n",
    "## Reset the environment\n",
    "env.reset()\n",
    "print(\"\\nInitial state : \",env.state)\n",
    "env = env.env\n",
    "# env.render()\n",
    "\n",
    "## Sample randomly one action\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action : \" , action)\n",
    "try :\n",
    "    state, reward, done, info = env.step(a = action)\n",
    "except :\n",
    "    state, reward, done, info = env.step(action = action)\n",
    "# env.render()\n",
    "print(\"\\nEpisode over : \",done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor and critic networks \n",
    "\n",
    "\n",
    "Questions : __/!\\ not defined precisely in the paper ??__\n",
    "\n",
    "NB : no parameter sharing between policy and value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Approximation of the value function V of a state given as input\n",
    "    FC network with 1 hidden layer and ReLU activations\n",
    "    Class used as 'critic'\n",
    "    Inputs : \n",
    "    input_size : dimension of each state\n",
    "    hidden_size : number of hidden layers\n",
    "    output_size : 1 (dimension of the value function estimate)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size = 1):\n",
    "        super(CustomValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x.float()))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self(x).detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0777], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "critic = CustomValueNetwork(env.observation_space.shape[0], 16, 1)\n",
    "critic(torch.tensor(env.state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDiscreteActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom policy model network for discrete action space\n",
    "    Inputs : \n",
    "    input_size : state space dimension\n",
    "    hidden_size : nb of hidden layers (64 in author's paper for continous action space)\n",
    "    action_size : action space dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(CustomDiscreteActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.fc1(x))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = F.softmax(self.fc3(out), dim=-1)\n",
    "        return out\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        return torch.multinomial(self(x), 1).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions :__\n",
    "* gaussian distribution for bounded actions ? \n",
    "* set std parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy model network for continuous action space (from the paper)\n",
    "    Inputs : \n",
    "    input_size : state space dimension\n",
    "    hidden_size : nb of hidden layers used by the authors\n",
    "    action_size : action space dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(ContinuousActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        self.std = torch.tensor(np.exp(-0.5 * np.ones(action_size))).float() #???\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.fc1(x.float()))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = torch.tanh(self.fc3(out))\n",
    "        return out\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        return torch.normal(self(x), self.std).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean :  tensor([0.3051, 0.3573, 0.3377], grad_fn=<SoftmaxBackward>)\n",
      "Sampled action :  [1]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "if 'n' in dir(env.action_space) : #check if we are in a discrete action space\n",
    "    actor = CustomDiscreteActorNetwork(env.observation_space.shape[0], 64, env.action_space.n)\n",
    "else :\n",
    "    actor = ContinuousActorNetwork(env.observation_space.shape[0], 64, env.action_space.shape[0])\n",
    "\n",
    "print(\"Mean : \", actor(torch.tensor(env.state)))\n",
    "print(\"Sampled action : \", actor.select_action(torch.tensor(env.state)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
